---
title: "PML-Assignment"
author: "Akio Tazuke"
date: "2015/5/24"
output: html_document
---

   The dataframe contained numerical, integer and factor variables. First, I tried a principal component preprocessing for numerical and integer variables. The contribution of even the first principal component was very low, suggesting the variables yield very week model. Also, there are also factor variables. This situation seemed to me that it's good to try the random forest. I chose 5 cross-validations. 
   Seen from the confusion matrix, the final model seems to be a very good one.
   So, I tried to predict the test data, but R said some variables are missing! So, I deleted colums in the training set that is missing data in the test set, and re-constructed the random forest as before. This time it took much time. Anyway, the result was as follows.

Random Forest 

19622 samples
   57 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold) 

Summary of sample sizes: 15697, 15697, 15699, 15697, 15698 

Resampling results across tuning parameters:
```{r, results='asis', echo = FALSE}
library(knitr)
mtry <- c(2, 40, 79)
acc <- c(0.991795, 0.9994394, 0.998624)
kap <- c(0.9896203, 0.999291, 0.9982596)
acSD <- c(0.0018759627, 0.0006836822, 0.0004262655)
kSD <- c(0.0023736589, 0.0008647619, 0.0005391463)
df <- data.frame(mtry = mtry, Accu = acc, Kappa = kap, AccuSD = acSD, KappaSD = kSD)
kable(df, format = "html", digits = 4)
```
Accuracy was used to select the optimal model using  the
 largest value.
The final value used for the model was mtry = 40. 

Call:
 randomForest(x = x, y = y, mtry = param$mtry, proximity = TRUE,      allowParallel = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 40

        OOB estimate of  error rate: 0.05%
Confusion matrix:
```{r, results='asis', echo = FALSE}
library(knitr)
aa <- c(5580, 1, 0, 0, 0)
bb <- c(0, 3796, 3, 0, 0)
cc <- c(0, 0, 3419, 2, 0)
dd <- c(0, 0, 0, 3213, 2)
ee <- c(0, 0, 0, 1, 3605)
Err <- c(0.0, 0.0002633658, 0.0008766803, 0.0009328358, 0.0005544774)
df <- data.frame(A = aa, B = bb, C = cc, D = dd, E = ee, Class.err = Err)
kable(df, format = "html", digits = 5)
```
   The reason of long waiting is now obvious. Because I deleted colums with missing values, there remained large number of complete rows, leading to a longer iteration. However, this seems to have improved the pridiction significantly. 
   So, I tried to predict the test data. However, the predict function said the data frame is not a required argument. I could not figure out why is this. So my report is incomplete. Time is running out and I got to go to bed for tomorrow's work...

